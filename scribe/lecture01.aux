\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{Hidden Variable Models}{1}{section*.1}}
\newlabel{sec:hidden-variable-models}{{}{1}{Hidden Variable Models}{section*.1}{}}
\@writefile{toc}{\contentsline {subsection}{Motivation}{1}{subsection*.2}}
\newlabel{sec:motivation}{{}{1}{Motivation}{subsection*.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Topic model is where each word is associated with a conditional topic. For example, it is more likely to observe word $w_i="Cat"$ if the topic is $z_i="Pets"$.}}{1}{equation.0.1}}
\newlabel{fig:topic-model-unigram}{{1}{1}{Motivation}{equation.0.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces In machine translation, each word from one language has a hidden alignment to its counterpart.}}{1}{Item.2}}
\newlabel{fig:machine-translation-alignment}{{2}{1}{Motivation}{Item.2}{}}
\@writefile{toc}{\contentsline {subsection}{Observed Case vs. Unobserved Case}{2}{subsection*.3}}
\newlabel{sec:observed-case-vs-unobserved-case}{{}{2}{Observed Case vs. Unobserved Case}{subsection*.3}{}}
\newlabel{eq:observed}{{2}{2}{Observed Case vs. Unobserved Case}{equation.0.2}{}}
\newlabel{eq:unobserved}{{3}{2}{Observed Case vs. Unobserved Case}{equation.0.3}{}}
\@writefile{toc}{\contentsline {section}{EM Algorithm}{2}{section*.4}}
\newlabel{sec:em-algorithm}{{}{2}{EM Algorithm}{section*.4}{}}
\newlabel{exmp1}{{1}{2}{}{exmp.1}{}}
\@writefile{toc}{\contentsline {subsection}{Observed Case}{3}{subsection*.5}}
\newlabel{sec:observed-case}{{}{3}{Observed Case}{subsection*.5}{}}
\@writefile{toc}{\contentsline {subsection}{Unobserved Case}{4}{subsection*.6}}
\newlabel{sec:unobserved-case}{{}{4}{Unobserved Case}{subsection*.6}{}}
\newlabel{priors}{{4}{4}{Unobserved Case}{equation.0.4}{}}
\newpmemlabel{^_1}{5}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces In the fully observed case (a), a single value of $y$ (which is the observed value itself) is associated with each $i^{\text  {th}}$ data sample. In the unobserved case (b), we assume that a entire list over the alphabet $\mathcal  {Y}$, is associated with $x_i$. Each element in the spectrum is assigned a weight, or the \textit  {fractional count}, which corresponds to the expected value of $y$ given $x_i$. The fractional count can also be understood as the confidence in the sample ($x_i, y_i$) given a prior belief over the parameters. }}{5}{subtable.1.2}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(a)}{\ignorespaces {$y$ is observed}}}{5}{subtable.1.1}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(b)}{\ignorespaces {$y$ is unobserved}}}{5}{subtable.1.2}}
\@writefile{toc}{\contentsline {subsection}{Properties of EM Algorithms}{6}{subsection*.7}}
\@writefile{toc}{\contentsline {section}{Topic Model}{6}{section*.8}}
\newlabel{sec:topic-model}{{}{6}{Topic Model}{section*.8}{}}
\newpmemlabel{^_2}{6}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces For each given topic, some words are more likely to appear than others}}{6}{section*.8}}
\@writefile{toc}{\contentsline {subsection}{Introduction to Topic Model}{6}{subsection*.9}}
\ttl@finishall
