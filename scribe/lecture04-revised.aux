\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{Hidden Variable Models}{1}{section*.1}}
\newlabel{sec:hidden-variable-models}{{}{1}{Hidden Variable Models}{section*.1}{}}
\@writefile{toc}{\contentsline {subsection}{Motivation}{1}{subsection*.2}}
\newlabel{sec:motivation}{{}{1}{Motivation}{subsection*.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Topic model is where each word is associated with a conditional topic. For example, it is more likely to observe word $w_i="Cat"$ if the topic is $z_i="Pets"$.}}{1}{equation.0.1}}
\newlabel{fig:topic-model-unigram}{{1}{1}{Motivation}{equation.0.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces In machine translation, each word from one language has a hidden alignment to its counterpart.}}{2}{Item.2}}
\newlabel{fig:machine-translation-alignment}{{2}{2}{Motivation}{Item.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Each word is associated with a hidden part-of-speech in English.}}{2}{Item.3}}
\newlabel{fig:part-of-speech}{{3}{2}{Motivation}{Item.3}{}}
\@writefile{toc}{\contentsline {subsection}{Observed Case vs. Unobserved Case}{2}{subsection*.3}}
\newlabel{sec:observed-case-vs-unobserved-case}{{}{2}{Observed Case vs. Unobserved Case}{subsection*.3}{}}
\newlabel{eq:observed}{{2}{2}{Observed Case vs. Unobserved Case}{equation.0.2}{}}
\newlabel{eq:unobserved}{{3}{2}{Observed Case vs. Unobserved Case}{equation.0.3}{}}
\newlabel{dog-cat-example}{{1}{3}{}{exmp.1}{}}
\@writefile{toc}{\contentsline {section}{EM Algorithm}{3}{section*.4}}
\newlabel{sec:em-algorithm}{{}{3}{EM Algorithm}{section*.4}{}}
\newlabel{em-example}{{2}{3}{}{exmp.2}{}}
\@writefile{toc}{\contentsline {subsection}{Observed Case}{4}{subsection*.5}}
\newlabel{sec:observed-case}{{}{4}{Observed Case}{subsection*.5}{}}
\@writefile{toc}{\contentsline {subsection}{Unobserved Case}{5}{subsection*.6}}
\newlabel{sec:unobserved-case}{{}{5}{Unobserved Case}{subsection*.6}{}}
\newlabel{priors}{{4}{5}{Unobserved Case}{equation.0.4}{}}
\newpmemlabel{^_1}{6}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces In the fully observed case (a), a single value of $y$ (which is the observed value itself) is associated with each $i^{\text  {th}}$ data sample. In the unobserved case (b), we assume that a entire list over the alphabet $\mathcal  {Y}$, is associated with $x_i$. Each element in the spectrum is assigned a weight, or the \textit  {fractional count}, which corresponds to the expected value of $y$ given $x_i$. The fractional count can also be understood as the confidence in the sample ($x_i, y_i$) given a prior belief over the parameters. }}{6}{subtable.1.2}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(a)}{\ignorespaces {$y$ is observed}}}{6}{subtable.1.1}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(b)}{\ignorespaces {$y$ is unobserved}}}{6}{subtable.1.2}}
\@writefile{toc}{\contentsline {subsection}{Properties of EM Algorithms}{7}{subsection*.7}}
\newpmemlabel{^_2}{7}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The coin example for $\mathbf  {y} = \{HHH, TTT, HHH, TTT\}$. The solution that EM reaches is intuitively correct: the coin-tosser has two coins, one which always shows up heads, the other which always shows tails, and is picking between them with equal probability $(\lambda = 0.5)$. The posterior probabilities $\mathaccentV {tilde}47E{p}_i$ show that we are certain on coin 1 (tail-biased) generated $y_2$ and $y_4$, whereas coin 2 generated $y_1$ and $y_3$.}}{7}{subsection*.7}}
\newlabel{em-good}{{4}{7}{Properties of EM Algorithms}{subsection*.7}{}}
\newpmemlabel{^_3}{7}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The coin example for $\mathbf  {y} = \{HHH, TTT, HHH, TTT\}$, with $p_1$ and $p_2$ initialized to the same value. EM is stuck at a saddle point. }}{7}{subsection*.7}}
\newlabel{em-bad}{{5}{7}{Properties of EM Algorithms}{subsection*.7}{}}
\@writefile{toc}{\contentsline {section}{Topic Model}{7}{section*.8}}
\newlabel{sec:topic-model}{{}{7}{Topic Model}{section*.8}{}}
\newpmemlabel{^_4}{7}
\@writefile{toc}{\contentsline {subsection}{Introduction to Topic Model}{7}{subsection*.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces For each given topic, some words are more likely to appear than others.}}{8}{section*.8}}
\ttl@finishall
