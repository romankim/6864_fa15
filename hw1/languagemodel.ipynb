{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# ----------------------------------\n",
    "def readCorpus(filename):\n",
    "    fp = open(filename, 'r')\n",
    "    corpus = []  # list of sentences. A sentence is a lists of words.\n",
    "    for line in fp:\n",
    "        line = line.strip()\n",
    "        # replace obvious numbers with <NUM>\n",
    "        line = re.sub(r'\\b\\d+\\b', r'<NUM>', line)\n",
    "        line = re.sub(r'\\b\\d+.\\d+\\b', r'<NUM>', line)\n",
    "        corpus.append(line.split(' '))\n",
    "    return corpus\n",
    "\n",
    "# ----------------------------------\n",
    "def buildIndex(corpus, lowthreshold=5):\n",
    "\n",
    "    # initial index, to be modified later\n",
    "    tmpindex, indx = {}, 0\n",
    "    for snt in corpus:\n",
    "        for w in snt:\n",
    "            if (not tmpindex.has_key(w)):\n",
    "                tmpindex[w] = indx\n",
    "                indx += 1\n",
    "\n",
    "    # eval word counts \n",
    "    counts = np.zeros((indx,))\n",
    "    for snt in corpus:\n",
    "        for w in snt:\n",
    "            counts[tmpindex[w]] += 1\n",
    "\n",
    "    # map all the words with counts leq lowthreshold to index 0\n",
    "    newindex = {}\n",
    "    indx = 1  # 0 reserved for low occurence words\n",
    "    for w in tmpindex.keys():\n",
    "        if (counts[tmpindex[w]] <= lowthreshold):\n",
    "            newindex[w] = 0\n",
    "        else:\n",
    "            newindex[w] = indx\n",
    "            indx += 1\n",
    "            \n",
    "    # add start symbols ... <START-2> <START-1> to the index for use with up to 5-grams\n",
    "    for j in range(1, 5):\n",
    "        newindex[\"<START-\" + str(j) + \">\"] = indx\n",
    "        indx += 1\n",
    "\n",
    "    return newindex, indx\n",
    "\n",
    "# ----------------------------------\n",
    "def ngramGen(corpus, w2index, n):\n",
    "    \"\"\"ngram generator. n is the length of the ngram.\"\"\"\n",
    "    assert(n <= 5)\n",
    "    ngrams = []\n",
    "    start_snt = [\"<START-\" + str(j) + \">\" for j in range(4, 0, -1)]\n",
    "    for snt in corpus:  # sentences\n",
    "        s = start_snt[-n + 1:] + snt\n",
    "        for i in xrange(n - 1, len(s)):\n",
    "            ngrams.append([w2index[w] for w in s[i - n + 1:i + 1]])\n",
    "    return ngrams\n",
    "\n",
    "# -----------------------------------\n",
    "def unigramLM(corpus, w2index, nwords):\n",
    "    uni  = ngramGen(corpus, w2index, 1)\n",
    "    prob = np.zeros((nwords,))\n",
    "    for w in uni:\n",
    "        prob[w[0]] += 1\n",
    "    return prob / float(np.sum(prob))\n",
    "\n",
    "# -----------------------------------\n",
    "def bigramLM(corpus, w2index, nwords, alpha=0.0):\n",
    "    bi   = ngramGen(corpus, w2index, 2)\n",
    "    prob = np.zeros((nwords,nwords))+alpha\n",
    "    for w in bi:\n",
    "        prob[w[0], w[1]] += 1.0\n",
    "    for i in xrange(nwords):\n",
    "        prob[i, :] /= np.sum(prob[i, :])\n",
    "    return prob\n",
    "\n",
    "# =====================================\n",
    "class softmax(object):\n",
    "    def __init__(self, dim, nwords):\n",
    "        self.nwords = nwords    # output dim\n",
    "        self.dim    = dim       # input dim       \n",
    "        self.Wo     = np.zeros((self.nwords,))\n",
    "        self.W      = np.random.randn(self.dim, self.nwords) / np.sqrt(self.dim)\n",
    "        self.prob   = np.ones((self.nwords,)) / float(self.nwords)\n",
    "        self.G2o    = 1e-12 * np.ones((self.nwords,)) # adagrad sum squared gradients for Wo\n",
    "        self.G2     = 1e-12 * np.ones((self.nwords,)) # adagrad sum squared gradients for W\n",
    "        \n",
    "    def apply(self, x):\n",
    "        z           = self.Wo + np.dot(x, self.W)\n",
    "        self.prob   = np.exp(z - np.max(z))\n",
    "        self.prob  /= np.sum(self.prob)\n",
    "        return self.prob\n",
    "\n",
    "    # update bias, accum wordvec gradient, return dlogP[y]/dx\n",
    "    def backprop(self, x, lrate, y):\n",
    "        grad       = -self.prob\n",
    "        grad[y]   += 1.0  # dlogP[y]/dz\n",
    "        xdelta     = np.dot(self.W, grad)  # dlogP[y]/dx\n",
    "        xnorm2     = np.sum(x ** 2)\n",
    "        self.G2o  += grad ** 2\n",
    "        self.G2   += xnorm2 * grad ** 2\n",
    "        self.Wo   += lrate * grad / np.sqrt(self.G2o)\n",
    "        self.W    += lrate * np.outer(x, grad / np.sqrt(self.G2))\n",
    "        return xdelta\n",
    "\n",
    "# =====================================\n",
    "class NNlayer(object):\n",
    "    def __init__(self, idim, odim):\n",
    "        self.idim = idim\n",
    "        self.odim = odim\n",
    "        self.W    = np.random.randn(self.idim, self.odim) / np.sqrt(self.idim)\n",
    "        self.Wo   = np.zeros(self.odim,)\n",
    "        # adaGrad sum squared gradients\n",
    "        self.G2o  = 1e-12 * np.ones((self.odim,))\n",
    "        self.G2   = 1e-12 * np.ones((self.odim,))\n",
    "        self.f    = np.zeros((self.odim,))  # activation of output units\n",
    "\n",
    "    def apply(self, x):\n",
    "        self.f = np.tanh(self.Wo + np.dot(x, self.W))\n",
    "        return self.f \n",
    "\n",
    "    def backprop(self, x, lrate, delta):\n",
    "        grad       = (1.0 - self.f ** 2) * delta  # dJ/dz = df/dz * delta.  (dtanh/dx = 1 - tanh^2) \n",
    "        xdelta     = np.dot(self.W, grad)         # dJ/dx to be returned\n",
    "        xnorm2     = np.sum(x ** 2)\n",
    "        self.G2o  += grad ** 2\n",
    "        self.G2   += xnorm2 * grad ** 2\n",
    "        self.Wo   += lrate * grad / np.sqrt(self.G2o)\n",
    "        self.W    += lrate * np.outer(x, grad / np.sqrt(self.G2))\n",
    "        return xdelta\n",
    "\n",
    "# =====================================\n",
    "class neuralLM(object):\n",
    "    def __init__(self, dim, ngram, hdim, nwords):\n",
    "        self.dim    = dim       # word vector dimension\n",
    "        self.ncond  = ngram - 1 # number of conditioning words\n",
    "        self.hdim   = hdim      # number of hidden layer units\n",
    "        self.nwords = nwords    # vocab size\n",
    "\n",
    "        self.wvec    = np.random.randn(self.nwords, self.dim)  # word vectors\n",
    "        self.G2      = 1e-12 * np.ones((self.nwords,))         # adaGrad sum of squares for word vectors\n",
    "        self.hiddenL = NNlayer(self.ncond * self.dim, self.hdim)\n",
    "        self.outputL = softmax(self.hdim, self.nwords)\n",
    "        \n",
    "    def prob(self, ngram):\n",
    "        xgram, y = ngram[:-1], ngram[-1]\n",
    "        x        = np.concatenate(self.wvec[xgram, :])\n",
    "        fh       = self.hiddenL.apply(x)\n",
    "        proba    = self.outputL.apply(fh)\n",
    "        return proba[y]\n",
    "\n",
    "    def update(self, ngram, lrate):\n",
    "        # Propagate (i.e. feed-forward pass)\n",
    "        xgram, y = ngram[:-1], ngram[-1]\n",
    "        x        = np.concatenate(self.wvec[xgram, :])\n",
    "        fh       = self.hiddenL.apply(x)\n",
    "        pr       = self.outputL.apply(fh)\n",
    "        # Backpropagate (and update layers)\n",
    "        dh       = self.outputL.backprop(fh, lrate, y)\n",
    "        dx       = self.hiddenL.backprop(x, lrate, dh)\n",
    "        # Update word vectors\n",
    "        grad     = np.reshape(dx, (self.ncond, self.dim))\n",
    "        for i in xrange(self.ncond):\n",
    "            self.G2[xgram[i]]      += np.sum(grad[i, :] ** 2)\n",
    "            self.wvec[xgram[i], :] += lrate * grad[i, :] / np.sqrt(self.G2[xgram[i]])\n",
    "\n",
    "        return pr[y]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
